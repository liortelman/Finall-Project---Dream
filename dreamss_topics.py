import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import text
from sklearn.decomposition import LatentDirichletAllocation
from pathlib import Path


def find_data_file():
    """
    ×¤×•× ×§×¦×™×” ×—×›××” ×©××—×¤×©×ª ××ª ×”×§×•×‘×¥.
    ×”×™× ××ª×—×™×œ×” ××”×ª×™×§×™×™×” ×©×‘×” ×”×¡×§×¨×™×¤×˜ ×¨×¥, ×•××—×¤×©×ª ×’× ×‘×ª×™×§×™×•×ª ×©××¢×œ×™×”.
    """
    # ××ª×—×™×œ×™× ××”×ª×™×§×™×™×” ×”× ×•×›×—×™×ª ×©×œ ×”×¡×§×¨×™×¤×˜
    current_dir = Path(__file__).resolve().parent
    filename = "all_dreams_combined.csv"

    print(f"ğŸ•µï¸ Searching for '{filename}' starting from: {current_dir}")

    # ×—×™×¤×•×© ×‘×œ×•×œ××”: ×‘×•×“×§ ×¤×”, ×•××– ×¢×•×œ×” ×ª×™×§×™×™×” ×œ××¢×œ×”, ×•×›×•' (×¢×“ 4 ×¨××•×ª)
    for _ in range(4):
        candidate = current_dir / filename
        if candidate.exists():
            print(f"âœ… Found file at: {candidate}")
            return candidate

        # ×× ×”×’×¢× ×• ×œ×©×•×¨×© ×”××—×©×‘, × ×¢×¦×•×¨
        if current_dir.parent == current_dir:
            break

        # ×¢×œ×™×™×” ×œ×ª×™×§×™×™×” ×©××¢×œ
        current_dir = current_dir.parent

    return None


def analyze_dreams_honest_model():
    # --- 1. ××¦×™××ª ×”×§×•×‘×¥ (×”×ª×™×§×•×Ÿ ×›××Ÿ) ---
    data_file = find_data_file()

    if not data_file:
        print("\nâŒ Error: Could not find 'all_dreams_combined.csv'.")
        print("Please make sure the file exists in your project folder.")
        return

    print(f"Loading data...")
    try:
        df = pd.read_csv(data_file)
    except Exception as e:
        print(f"Error reading CSV: {e}")
        return

    # --- 2. ××¦×™××ª ×¢××•×“×ª ×”×˜×§×¡×˜ ---
    text_col = next((col for col in ['report', 'content', 'dream', 'description'] if col in df.columns), None)
    if not text_col:
        # ×× ×œ× ××•×¦× ×œ×¤×™ ×©×, ×œ×•×§×— ××ª ×”×¢××•×“×” ×¢× ×”×˜×§×¡×˜ ×”×›×™ ××¨×•×š
        text_col = max(df.select_dtypes(include=['object']), key=lambda c: df[c].astype(str).str.len().mean())

    df = df.dropna(subset=[text_col])
    print(f"Analyzing {len(df)} dreams using column '{text_col}'...")

    # --- 3. × ×™×§×•×™ ×•×¡×™× ×•×Ÿ (Stop Words) ---
    custom_stop_words = list(text.ENGLISH_STOP_WORDS)
    custom_stop_words.extend([
        'dream', 'dreamed', 'dreamt', 'woke', 'awakened', 'remember', 'recall','say', 'says', 'look'
        'said', 'went', 'got', 'did', 'like', 'just', 'know', 'think', 'saw', 'felt','didn', 'thought', 'don'
        'asked', 'told', 'started', 'looking', 'going', 'looked', 'wanted', 'came','couldn','saying', 'sent'
    ])

    # --- 4. ×‘× ×™×™×ª ×”××•×“×œ ---
    tf_vectorizer = CountVectorizer(
        max_df=0.90,
        min_df=10,
        stop_words=custom_stop_words,
        max_features=1500,
        token_pattern=r'(?u)\b[a-zA-Z]{3,}\b'
    )

    tf = tf_vectorizer.fit_transform(df[text_col].astype(str))

    n_topics = 50
    print(f"Running model to find {n_topics} natural clusters...")

    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda.fit(tf)

    # --- 5. ×™×¦×™×¨×ª ×©××•×ª ×œ×§×˜×’×•×¨×™×•×ª ×‘××•×¤×Ÿ ××•×˜×•××˜×™ ---
    feature_names = tf_vectorizer.get_feature_names_out()
    topic_labels = {}

    print("\n--- Identifying Categories (Generated by Model) ---")
    for topic_idx, topic in enumerate(lda.components_):
        # ×œ×•×§×— ××ª 3 ×”××™×œ×™× ×”×›×™ ×—×–×§×•×ª ×‘×§×˜×’×•×¨×™×”
        top_words_idx = topic.argsort()[:-4:-1]
        top_words = [feature_names[i] for i in top_words_idx]

        auto_label = ", ".join(top_words)
        topic_labels[topic_idx] = auto_label

        # print(f"Topic {topic_idx + 1}: {auto_label}") # Removed to avoid clutter, will print in stats

    # --- 6. ×©×™×•×š ×•×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×” ---
    topic_values = lda.transform(tf)
    # Note: Topic ID is 1-based here for readability
    df['topic_id'] = topic_values.argmax(axis=1) + 1
    # Note: We map using the 0-based index from lda.components_
    df['topic_keywords'] = (df['topic_id'] - 1).map(topic_labels)

    print("\n" + "=" * 60)
    print("       ğŸ“Š DREAM STATISTICS (AUTO-DETECTED WITH ID)")
    print("=" * 60)

    # Group by both ID and Keywords to keep the ID available
    stats = df.groupby(['topic_id', 'topic_keywords']).size().reset_index(name='count')

    # Sort by count descending (most common topics first)
    stats = stats.sort_values(by='count', ascending=False)

    total = len(df)

    for _, row in stats.iterrows():
        topic_id = row['topic_id']
        category = row['topic_keywords']
        count = row['count']
        percent = (count / total) * 100

        # Added Topic ID to the print statement
        print(f"ğŸ“‚ [Topic {topic_id:<2}] {category:<30} : {count} dreams ({percent:.1f}%)")

    print("=" * 60)

    # --- 7. ×©××™×¨×” ×œ×§×•×‘×¥ ---
    output_path = data_file.parent / "dreams_auto_categorized.csv"
    df.to_csv(output_path, index=False)
    print(f"\nâœ… SUCCESS! File saved at:\n{output_path}")


if __name__ == "__main__":
    analyze_dreams_honest_model()