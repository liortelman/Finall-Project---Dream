import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import text
from sklearn.decomposition import LatentDirichletAllocation
from pathlib import Path


def analyze_dreams_honest_model():
    # --- 1. ×˜×¢×™× ×ª ×”× ×ª×•× ×™× ---
    current_script_path = Path(__file__).resolve()
    project_root = current_script_path.parents[2]

    possible_files = [project_root / "all_dreams_combined.csv", project_root / "all_dreams_combined_json.csv"]
    data_file = next((f for f in possible_files if f.exists()), None)

    if not data_file:
        print("Error: Combined CSV not found.")
        return

    print(f"Loading data from {data_file.name}...")
    df = pd.read_csv(data_file)

    text_col = next((col for col in ['report', 'content', 'dream', 'description'] if col in df.columns), None)
    if not text_col:
        text_col = max(df.select_dtypes(include=['object']), key=lambda c: df[c].astype(str).str.len().mean())

    df = df.dropna(subset=[text_col])

    # --- 2. × ×™×§×•×™ ×•×¡×™× ×•×Ÿ ---
    custom_stop_words = list(text.ENGLISH_STOP_WORDS)
    custom_stop_words.extend([
        'dream', 'dreamed', 'dreamt', 'woke', 'awakened', 'remember', 'recall',
        'said', 'went', 'got', 'did', 'like', 'just', 'know', 'think', 'saw', 'felt',
        'asked', 'told', 'started', 'looking', 'going', 'looked', 'wanted', 'came',
        'ich', 'und', 'die', 'der', 'das', 'ein', 'zu', 'war', 'nicht', 'mit', 'den', 'auf', 'ist'
    ])

    print(f"Analyzing {len(df)} dreams...")

    # --- 3. ×‘× ×™×™×ª ×”××•×“×œ ---
    tf_vectorizer = CountVectorizer(
        max_df=0.90,
        min_df=10,
        stop_words=custom_stop_words,
        max_features=1500,
        token_pattern=r'(?u)\b[a-zA-Z]{3,}\b'
    )

    tf = tf_vectorizer.fit_transform(df[text_col].astype(str))

    # ××¤×©×¨ ×œ×©× ×•×ª ××ª ××¡×¤×¨ ×”× ×•×©××™× ×œ-6 ××• 8 ×›×“×™ ×œ×§×‘×œ ×—×œ×•×§×” ××“×•×™×§×ª ×™×•×ª×¨
    n_topics = 500
    print(f"Running model to find {n_topics} natural clusters...")

    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda.fit(tf)

    # --- 4. ×™×¦×™×¨×ª ×©××•×ª ×œ×§×˜×’×•×¨×™×•×ª ×‘××•×¤×Ÿ ××•×˜×•××˜×™ ---
    # ×›××Ÿ ×”×©×™× ×•×™ ×”×’×“×•×œ: ×× ×—× ×• ×œ× ×××¦×™××™× ×©××•×ª. ×”××•×“×œ ×§×•×‘×¢ ××ª ×”×©× ×œ×¤×™ ×”××™×œ×™× ×”××•×‘×™×œ×•×ª.
    feature_names = tf_vectorizer.get_feature_names_out()
    topic_labels = {}

    print("\n--- Identifying Categories (Generated by Model) ---")
    for topic_idx, topic in enumerate(lda.components_):
        # ×œ×•×§×— ××ª 3 ×”××™×œ×™× ×”×›×™ ×—×–×§×•×ª ×‘×§×˜×’×•×¨×™×”
        top_words_idx = topic.argsort()[:-4:-1]
        top_words = [feature_names[i] for i in top_words_idx]

        # ×”×©× ×©×œ ×”×§×˜×’×•×¨×™×” ×”×•× ×¤×©×•×˜ ×”××™×œ×™× ×¢×¦××Ÿ, ×œ××©×œ: "school, class, test"
        auto_label = ", ".join(top_words)
        topic_labels[topic_idx] = auto_label

        print(f"Topic {topic_idx + 1}: {auto_label}")

    # --- 5. ×©×™×•×š ×•×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×” ---
    topic_values = lda.transform(tf)
    df['topic_id'] = topic_values.argmax(axis=1) + 1

    # ×©×•××¨×™× ××ª ×”×©× ×”××•×˜×•××˜×™ ×‘×¢××•×“×”
    df['topic_keywords'] = (df['topic_id'] - 1).map(topic_labels)

    # ×”×“×¤×¡×ª ×”×¡×˜×˜×™×¡×˜×™×§×”
    print("\n" + "=" * 50)
    print("       ğŸ“Š DREAM STATISTICS (AUTO-DETECTED)")
    print("=" * 50)

    category_counts = df['topic_keywords'].value_counts()
    total = len(df)

    for category, count in category_counts.items():
        percent = (count / total) * 100
        # ××“×¤×™×¡: 3 ×”××™×œ×™× ×”××•×‘×™×œ×•×ª | ×›××•×ª | ××—×•×–
        print(f"ğŸ“‚ [{category:<30}] : {count} dreams ({percent:.1f}%)")

    print("=" * 50)

    # --- 6. ×©××™×¨×” ×œ×§×•×‘×¥ ---
    output_path = project_root / "dreams_auto_categorized.csv"
    df.to_csv(output_path, index=False)
    print(f"\nâœ… SUCCESS! File saved at:\n{output_path}")


if __name__ == "__main__":
    analyze_dreams_honest_model()